{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stuna-alice/flutter_app_practice_v1/blob/master/C_backend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlC6AYFHzdkZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import subprocess\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# Set the working directory\n",
        "working_directory = '/content/gdrive/MyDrive/Winforms_data/yolov5'\n",
        "os.chdir(working_directory)\n",
        "\n",
        "# Function to read parameters from the text file\n",
        "def read_parameters(file_path):\n",
        "    parameters = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            # Check if the line contains a colon\n",
        "            if ':' in line:\n",
        "                key, value = line.strip().split(':', 1)\n",
        "                parameters[key.strip()] = value.strip()\n",
        "    return parameters\n",
        "\n",
        "# Define the zip file name\n",
        "zip_file_name = 'chickenDB.zip'\n",
        "\n",
        "# Extract the base name of the zip file (excluding the extension)\n",
        "base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
        "\n",
        "# Update the YAML file with the extracted base name\n",
        "yaml_content = f\"\"\"\n",
        "train: {working_directory}/dataset_extract/{base_name}/images/train/\n",
        "val: {working_directory}/dataset_extract/{base_name}/images/val/\n",
        "nc: 1\n",
        "names: ['{base_name}']\n",
        "\"\"\"\n",
        "\n",
        "# Write the updated YAML content to a file\n",
        "yaml_file_path = f'{working_directory}/data/defaultYamlpath.yaml'\n",
        "with open(yaml_file_path, 'w') as yaml_file:\n",
        "    yaml_file.write(yaml_content)\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "extracted_folder_path = f'{working_directory}/dataset_extract'\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the zip file and the destination folder\n",
        "zip_file_path = f'{working_directory}/zipfiles/{zip_file_name}'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(f\"'{zip_file_name}' extracted to '{extracted_folder_path}'.\")\n",
        "\n",
        "# Read user parameters from the text file\n",
        "text_file_path = f'{working_directory}/zipfiles/{base_name}.txt'\n",
        "if not os.path.exists(text_file_path):\n",
        "    print(f\"Error: Text file '{base_name}.txt' not found.\")\n",
        "else:\n",
        "    parameters = read_parameters(text_file_path)\n",
        "\n",
        "    # Print all parameters\n",
        "    print(\"All Parameters:\")\n",
        "    for key, value in parameters.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    # Check if required parameters are present\n",
        "    required_params = ['img', 'batch', 'epochs', 'weights']\n",
        "    missing_params = [param for param in required_params if param not in parameters]\n",
        "\n",
        "    if missing_params:\n",
        "        print(f\"Error: Missing parameters in text file: {', '.join(missing_params)}\")\n",
        "    else:\n",
        "        # Train YOLOv5s with user parameters\n",
        "        weights_path = parameters['weights']\n",
        "        !python train.py --img {parameters['img']} --batch {parameters['batch']} --epochs {parameters['epochs']} --data {yaml_file_path} --weights {weights_path} --cache\n",
        "\n",
        "        # Check if the export.py script exists\n",
        "        export_script_path = os.path.join(working_directory, 'export.py')\n",
        "        if not os.path.exists(export_script_path):\n",
        "            print(f\"Error: 'export.py' not found in the specified directory.\")\n",
        "        else:\n",
        "          # Export the trained model to ONNX format\n",
        "          output_onnx_path = f'{working_directory}/{base_name}.onnx'\n",
        "          export_cmd = f\"python {export_script_path} --weights {weights_path} --img-size {parameters['img']} --dynamic --simplify --optimize 0 {output_onnx_path}\"\n",
        "\n",
        "          # Run the export command and capture output and errors\n",
        "          try:\n",
        "              onnx_export_output = subprocess.check_output(export_cmd, stderr=subprocess.STDOUT, text=True, shell=True)\n",
        "              print(f\"ONNX file exported to '{output_onnx_path}'.\")\n",
        "          except subprocess.CalledProcessError as e:\n",
        "              print(f\"Error: Export command failed with code {e.returncode}.\")\n",
        "              print(f\"Export command output:\\n{e.output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x18KrmCb4JFe",
        "outputId": "dcdac1fa-07da-424a-a7f2-6f6f4748c72f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "'chickenDB.zip' extracted to '/content/gdrive/MyDrive/Winforms_data/yolov5/dataset_extract'.\n",
            "All Parameters:\n",
            "img: 640\n",
            "batch: 20\n",
            "epochs: 2\n",
            "weights: yolov5s.pt\n",
            "Training command: python train.py --img 640 --batch 20 --epochs 2 --data /content/gdrive/MyDrive/Winforms_data/yolov5/data/defaultYamlpath.yaml --weights yolov5s.pt --cache\n",
            "Exporting model to ONNX. Command: python export.py --weights yolov5s.pt --img-size 640 --include pb --dynamic --simplify --optimize 0 --simplify-num-ends 0 --simplify-num-mid 0 --simplify-num-start 0 --simplify-threshold 0.5 --simplify-method 0 --simplify-img-size 640 --simplify-nc 1 --dynamic-nc --simplify-preserve-reduction --output /content/gdrive/MyDrive/Winforms_data/yolov5/zipfiles/chickenDB.onnx\n",
            "Export successful.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#running\n",
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import subprocess\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# Set the working directory\n",
        "working_directory = '/content/gdrive/MyDrive/Winforms_data/yolov5'\n",
        "os.chdir(working_directory)\n",
        "\n",
        "# Function to read parameters from the text file\n",
        "def read_parameters(file_path):\n",
        "    parameters = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            # Check if the line contains a colon\n",
        "            if ':' in line:\n",
        "                key, value = line.strip().split(':', 1)\n",
        "                parameters[key.strip()] = value.strip()\n",
        "    return parameters\n",
        "\n",
        "# Define the zip file name\n",
        "zip_file_name = 'chickenDB.zip'\n",
        "\n",
        "# Extract the base name of the zip file (excluding the extension)\n",
        "base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
        "\n",
        "# Update the YAML file with the extracted base name\n",
        "yaml_content = f\"\"\"\n",
        "train: {working_directory}/dataset_extract/{base_name}/images/train/\n",
        "val: {working_directory}/dataset_extract/{base_name}/images/val/\n",
        "nc: 1\n",
        "names: ['{base_name}']\n",
        "\"\"\"\n",
        "\n",
        "# Write the updated YAML content to a file\n",
        "yaml_file_path = f'{working_directory}/data/defaultYamlpath.yaml'\n",
        "with open(yaml_file_path, 'w') as yaml_file:\n",
        "    yaml_file.write(yaml_content)\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "extracted_folder_path = f'{working_directory}/dataset_extract'\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the zip file and the destination folder\n",
        "zip_file_path = f'{working_directory}/zipfiles/{zip_file_name}'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(f\"'{zip_file_name}' extracted to '{extracted_folder_path}'.\")\n",
        "\n",
        "# Read user parameters from the text file\n",
        "text_file_path = f'{working_directory}/zipfiles/{base_name}.txt'\n",
        "if not os.path.exists(text_file_path):\n",
        "    print(f\"Error: Text file '{base_name}.txt' not found.\")\n",
        "else:\n",
        "    parameters = read_parameters(text_file_path)\n",
        "\n",
        "    # Print all parameters\n",
        "    print(\"All Parameters:\")\n",
        "    for key, value in parameters.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    # Check if required parameters are present\n",
        "    required_params = ['img', 'batch', 'epochs', 'weights']\n",
        "    missing_params = [param for param in required_params if param not in parameters]\n",
        "\n",
        "    if missing_params:\n",
        "        print(f\"Error: Missing parameters in text file: {', '.join(missing_params)}\")\n",
        "    else:\n",
        "        # Train YOLOv5s with user parameters\n",
        "        weights_path = parameters['weights']\n",
        "        training_cmd = \"python train.py --img {img} --batch {batch} --epochs {epochs} --data {data} --weights {weights} --cache\".format(\n",
        "            img=parameters['img'],\n",
        "            batch=parameters['batch'],\n",
        "            epochs=parameters['epochs'],\n",
        "            data=yaml_file_path,\n",
        "            weights=weights_path\n",
        "        )\n",
        "        print(f\"Training command: {training_cmd}\")\n",
        "\n",
        "        # Execute training command using subprocess.run\n",
        "        subprocess.run(training_cmd, shell=True)\n",
        "\n",
        "        # Export the trained model to ONNX format\n",
        "        output_onnx_path = f'{working_directory}/zipfiles/{base_name}.onnx'\n",
        "        onnx_export_cmd = \"python export.py --weights {weights} --img-size {img_size} --include pb --dynamic --simplify --optimize 0 --simplify-num-ends 0 --simplify-num-mid 0 --simplify-num-start 0 --simplify-threshold 0.5 --simplify-method 0 --simplify-img-size {img_size} --simplify-nc 1 --dynamic-nc --simplify-preserve-reduction --output {output_path}\".format(\n",
        "            weights=weights_path,\n",
        "            img_size=parameters['img'],\n",
        "            output_path=output_onnx_path\n",
        "        )\n",
        "\n",
        "        print(f\"Exporting model to ONNX. Command: {onnx_export_cmd}\")\n",
        "\n",
        "        try:\n",
        "            # Execute ONNX export command using subprocess.run\n",
        "            subprocess.run(onnx_export_cmd, shell=True)\n",
        "            print(f\"Export successful.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during ONNX export: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRoO7Kz9qSI7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import subprocess\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# Set the working directory\n",
        "working_directory = '/content/gdrive/MyDrive/Winforms_data/yolov5'\n",
        "os.chdir(working_directory)\n",
        "\n",
        "# Function to read parameters from the text file\n",
        "def read_parameters(file_path):\n",
        "    parameters = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            # Check if the line contains a colon\n",
        "            if ':' in line:\n",
        "                key, value = line.strip().split(':', 1)\n",
        "                parameters[key.strip()] = value.strip()\n",
        "    return parameters\n",
        "\n",
        "# Define the zip file name\n",
        "zip_file_name = 'chickenDB.zip'\n",
        "\n",
        "# Extract the base name of the zip file (excluding the extension)\n",
        "base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
        "\n",
        "# Update the YAML file with the extracted base name\n",
        "yaml_content = f\"\"\"\n",
        "train: {working_directory}/dataset_extract/{base_name}/images/train/\n",
        "val: {working_directory}/dataset_extract/{base_name}/images/val/\n",
        "nc: 1\n",
        "names: ['{base_name}']\n",
        "\"\"\"\n",
        "\n",
        "# Write the updated YAML content to a file\n",
        "yaml_file_path = f'{working_directory}/data/defaultYamlpath.yaml'\n",
        "with open(yaml_file_path, 'w') as yaml_file:\n",
        "    yaml_file.write(yaml_content)\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "extracted_folder_path = f'{working_directory}/dataset_extract'\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the zip file and the destination folder\n",
        "zip_file_path = f'{working_directory}/zipfiles/{zip_file_name}'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(f\"'{zip_file_name}' extracted to '{extracted_folder_path}'.\")\n",
        "\n",
        "# Read user parameters from the text file\n",
        "text_file_path = f'{working_directory}/zipfiles/{base_name}.txt'\n",
        "if not os.path.exists(text_file_path):\n",
        "    print(f\"Error: Text file '{base_name}.txt' not found.\")\n",
        "else:\n",
        "    parameters = read_parameters(text_file_path)\n",
        "\n",
        "    # Print all parameters\n",
        "    print(\"All Parameters:\")\n",
        "    for key, value in parameters.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    # Check if required parameters are present\n",
        "    required_params = ['img', 'batch', 'epochs', 'weights']\n",
        "    missing_params = [param for param in required_params if param not in parameters]\n",
        "\n",
        "    if missing_params:\n",
        "        print(f\"Error: Missing parameters in text file: {', '.join(missing_params)}\")\n",
        "    else:\n",
        "        # Train YOLOv5s with user parameters\n",
        "        weights_path = parameters['weights']\n",
        "        !python train.py --img {parameters['img']} --batch {parameters['batch']} --epochs {parameters['epochs']} --data {yaml_file_path} --weights {weights_path} --cache\n",
        "\n",
        "        # Check if the export.py script exists\n",
        "        export_script_path = os.path.join(working_directory, 'export.py')\n",
        "        if not os.path.exists(export_script_path):\n",
        "            print(f\"Error: 'export.py' not found in the specified directory.\")\n",
        "        else:\n",
        "            # Export the trained model to ONNX format\n",
        "            output_onnx_path = f'{working_directory}/{base_name}.onnx'\n",
        "            onnx_export_cmd = f\"python {export_script_path} --weights {weights_path} --img-size {parameters['img']} --include pb --dynamic --simplify --optimize 0 --simplify-num-ends 0 --simplify-num-mid 0 --simplify-num-start 0 --simplify-threshold 0.5 --simplify-method 0 --simplify-img-size {parameters['img']} --simplify-nc 1 --dynamic-nc --simplify-preserve-reduction --output {output_onnx_path}\"\n",
        "\n",
        "            # Check if the export command was successful\n",
        "            exit_code = os.system(onnx_export_cmd)\n",
        "\n",
        "            # Capture the output of the command\n",
        "            onnx_export_output = os.popen(onnx_export_cmd).read()\n",
        "\n",
        "            if exit_code == 0:\n",
        "                print(f\"ONNX file exported to '{output_onnx_path}'.\")\n",
        "            else:\n",
        "                print(f\"Error: Export command exited with code {exit_code}.\")\n",
        "                print(f\"Export command output:\\n{onnx_export_output}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpAv5sHVjaJA"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# Set the working directory\n",
        "working_directory = '/content/gdrive/MyDrive/Winforms_data/yolov5'\n",
        "os.chdir(working_directory)\n",
        "\n",
        "# Function to read parameters from the text file\n",
        "def read_parameters(file_path):\n",
        "    parameters = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            key, value = line.strip().split(':')\n",
        "            parameters[key.strip()] = value.strip()\n",
        "    return parameters\n",
        "\n",
        "# Define the zip file name\n",
        "zip_file_name = 'chickenDB.zip'\n",
        "\n",
        "# Extract the base name of the zip file (excluding the extension)\n",
        "base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
        "\n",
        "# Update the YAML file with the extracted base name\n",
        "yaml_content = f\"\"\"\n",
        "train: {working_directory}/dataset_extract/{base_name}/images/train/\n",
        "val: {working_directory}/dataset_extract/{base_name}/images/val/\n",
        "nc: 1\n",
        "names: ['{base_name}']\n",
        "\"\"\"\n",
        "\n",
        "# Write the updated YAML content to a file\n",
        "yaml_file_path = f'{working_directory}/data/defaultYamlpath.yaml'\n",
        "with open(yaml_file_path, 'w') as yaml_file:\n",
        "    yaml_file.write(yaml_content)\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "extracted_folder_path = f'{working_directory}/dataset_extract'\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the zip file and the destination folder\n",
        "zip_file_path = f'{working_directory}/zipfiles/{zip_file_name}'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(f\" '{zip_file_name}' extracted to '{extracted_folder_path}'.\")\n",
        "\n",
        "# Read user parameters from the text file\n",
        "text_file_path = f'{working_directory}/zipfiles/{base_name}.txt'\n",
        "if not os.path.exists(text_file_path):\n",
        "    print(f\"Error: Text file '{base_name}.txt' not found.\")\n",
        "else:\n",
        "    parameters = read_parameters(text_file_path)\n",
        "\n",
        "    # Check if required parameters are present\n",
        "    required_params = ['img', 'batch', 'epochs', 'weights']\n",
        "    missing_params = [param for param in required_params if param not in parameters]\n",
        "\n",
        "    if missing_params:\n",
        "        print(f\"Error: Missing parameters in text file: {', '.join(missing_params)}\")\n",
        "    else:\n",
        "        # Train YOLOv5s with user parameters\n",
        "        !python train.py --img {parameters['img']}  --batch {parameters['batch']} --epochs {parameters['epochs']} --data {yaml_file_path} --weights {parameters['weights']} --cache\n",
        "\n",
        "        # Export the trained model to ONNX format\n",
        "        onnx_file_name = f'{base_name}.onnx'\n",
        "        output_onnx_path = f'{working_directory}/{onnx_file_name}'\n",
        "        export_command = f\"python export.py --weights runs/train/exp/weights/best.pt --img-size {parameters['img']} --batch-size {parameters['batch']} --include pb\"  # Modify the export.py command based on your requirements\n",
        "\n",
        "        !{export_command}\n",
        "\n",
        "        # Move the exported ONNX file to the desired location\n",
        "        shutil.move('yolov5s.onnx', output_onnx_path)\n",
        "\n",
        "        print(f\"Trained model exported to '{output_onnx_path}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eQ-3FBot291L"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Change directory to allow Google to access GDrive\n",
        "%cd /content/gdrive/MyDrive/Winforms_data\n",
        "\n",
        "# Clone YOLOv5 repository\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "\n",
        "# Change directory to the YOLOv5 repository\n",
        "%cd yolov5\n",
        "\n",
        "# Install required packages\n",
        "%pip install -qr requirements.txt comet_ml\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "\n",
        "# Define the path to the zip file and the destination folder\n",
        "zipFile_name = 'chickenDB.zip'\n",
        "zip_file_path = '/content/gdrive/MyDrive/Winforms_data/yolov5/zipfiles/' + zipFile_name\n",
        "extracted_folder_path = '/content/gdrive/MyDrive/Winforms_data/yolov5/dataset_extract'\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(f\" '{zipFile_name}' extracted to '{extracted_folder_path}'.\")\n",
        "\n",
        "\n",
        "# make label code\n",
        "#training\n",
        "# Train YOLOv5s on marmot for 3 epochs\n",
        "!python train.py --img 640 --batch 20 --epochs 50 --data controlDB.yaml --weights yolov5s.pt --cache\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4xUYGnB7hSSQ",
        "outputId": "f3de5ddb-8b3b-40b9-9a71-2bc1b29fb4b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 üöÄ v7.0-241-gb6a65e1 Python-3.10.12 torch-2.1.0+cu121 CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 26.7/107.7 GB disk)\n",
            " 'chickenDB.zip' extracted to '/content/gdrive/MyDrive/Winforms_data/yolov5/dataset_extract'.\n",
            "2023-12-19 07:05:43.336924: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-19 07:05:43.336998: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-19 07:05:43.339292: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/gdrive/MyDrive/Winforms_data/yolov5/data/defaultYamlpath.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=2, batch_size=21, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 12 commits. Use 'git pull' or 'git clone https://github.com/ultralytics/yolov5' to update.\n",
            "YOLOv5 üöÄ v7.0-241-gb6a65e1 Python-3.10.12 torch-2.1.0+cu121 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "COMET WARNING: Comet credentials have not been set. Comet will default to offline logging. Please set your credentials to enable online logging.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Using '/content/gdrive/MyDrive/Winforms_data/yolov5/.cometml-runs' path as offline directory. Pass 'offline_directory' parameter into constructor or set the 'COMET_OFFLINE_DIRECTORY' environment variable to manually choose where to store offline experiment archives.\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0004921875), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/Winforms_data/yolov5/dataset_extract/chickenDB/labels/train.cache... 46 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46/46 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46/46 [00:00<00:00, 60.47it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/Winforms_data/yolov5/dataset_extract/chickenDB/labels/val.cache... 28 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:01<00:00, 27.97it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m3.12 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
            "Plotting labels to runs/train/exp162/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp162\u001b[0m\n",
            "Starting training for 2 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        0/1         0G     0.1096    0.03566          0         11        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:37<00:00, 32.57s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/1 [00:00<?, ?it/s]WARNING ‚ö†Ô∏è NMS time limit 1.900s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:18<00:00, 18.02s/it]\n",
            "                   all         28         43    0.00111     0.0233   0.000579   0.000116\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        1/1         0G     0.1113    0.03552          0         12        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:25<00:00, 28.49s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/1 [00:00<?, ?it/s]WARNING ‚ö†Ô∏è NMS time limit 1.900s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:16<00:00, 16.60s/it]\n",
            "                   all         28         43    0.00222     0.0465    0.00117   0.000174\n",
            "\n",
            "2 epochs completed in 0.061 hours.\n",
            "Optimizer stripped from runs/train/exp162/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from runs/train/exp162/weights/best.pt, 14.4MB\n",
            "\n",
            "Validating runs/train/exp162/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/1 [00:00<?, ?it/s]WARNING ‚ö†Ô∏è NMS time limit 1.900s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:15<00:00, 15.71s/it]\n",
            "                   all         28         43    0.00222     0.0465    0.00117   0.000174\n",
            "Results saved to \u001b[1mruns/train/exp162\u001b[0m\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml OfflineExperiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : [OfflineExperiment will get URL after upload]\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss                     : 3.3719587326049805\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/mAP_0.5 [4]      : (0.0005790287434673938, 0.0011657390560175626)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/mAP_0.5:0.95 [4] : (0.00011580574869347875, 0.0001738807660891867)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/precision [4]    : (0.0011111111111111111, 0.0022222222222222222)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/recall [4]       : (0.023255813953488372, 0.046511627906976744)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/box_loss [4]       : (0.10956879705190659, 0.11131229251623154)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/cls_loss           : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/obj_loss [4]       : (0.035520654171705246, 0.035661954432725906)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/box_loss [4]         : (0.13103413581848145, 0.13231782615184784)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/cls_loss             : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/obj_loss [4]         : (0.027587655931711197, 0.028480717912316322)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     x/lr0 [4]                : (0.0952525, 0.09820000000000001)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     x/lr1 [4]                : (0.0002, 0.0002525)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     x/lr2 [4]                : (0.0002, 0.0002525)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name                        : exp\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     comet_log_batch_metrics     : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     comet_log_confusion_matrix  : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     comet_log_per_class_metrics : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     comet_max_image_uploads     : 100\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     comet_mode                  : online\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     comet_model_name            : yolov5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hasNestedParams             : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     offline_experiment          : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     anchor_t            : 4.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     artifact_alias      : latest\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size          : 21\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     bbox_interval       : -1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     box                 : 0.05\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     bucket              : \n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cfg                 : \n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cls                 : 0.006250000000000001\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cls_pw              : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     copy_paste          : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cos_lr              : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     degrees             : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     device              : \n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     entity              : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     evolve              : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     exist_ok            : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fl_gamma            : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fliplr              : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flipud              : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     freeze              : [0]\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_h               : 0.015\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_s               : 0.7\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_v               : 0.4\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|anchor_t        : 4.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|box             : 0.05\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|cls             : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|cls_pw          : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|copy_paste      : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|degrees         : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|fl_gamma        : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|fliplr          : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|flipud          : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|hsv_h           : 0.015\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|hsv_s           : 0.7\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|hsv_v           : 0.4\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|iou_t           : 0.2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|lr0             : 0.01\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|lrf             : 0.01\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|mixup           : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|momentum        : 0.937\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|mosaic          : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|obj             : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|obj_pw          : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|perspective     : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|scale           : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|shear           : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|translate       : 0.1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|warmup_bias_lr  : 0.1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|warmup_epochs   : 3.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|warmup_momentum : 0.8\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|weight_decay    : 0.0005\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     image_weights       : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     imgsz               : 640\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     iou_t               : 0.2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     label_smoothing     : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     local_rank          : -1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr0                 : 0.01\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lrf                 : 0.01\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mixup               : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     momentum            : 0.937\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mosaic              : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     multi_scale         : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                : exp\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     noautoanchor        : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     noplots             : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     nosave              : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     noval               : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     obj                 : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     obj_pw              : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer           : SGD\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     patience            : 100\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     perspective         : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     project             : runs/train\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     quad                : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     rect                : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     resume              : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_dir            : runs/train/exp162\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_period         : -1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     scale               : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     seed                : 0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     shear               : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     single_cls          : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     sync_bn             : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     translate           : 0.1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     upload_dataset      : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_conf_threshold  : 0.001\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_iou_threshold   : 0.6\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_bias_lr      : 0.1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_epochs       : 3.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_momentum     : 0.8\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     weight_decay        : 0.0004921875\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     workers             : 8\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     asset               : 13 (967.61 KB)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     confusion-matrix    : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata        : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     images              : 8\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still saving offline stats to messages file before program termination (may take up to 120 seconds)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Starting saving the offline archive\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m To upload this offline experiment, run:\n",
            "    comet upload /content/gdrive/MyDrive/Winforms_data/yolov5/.cometml-runs/2849d9431407407db1a95afe8fd80976.zip\n",
            "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=yolov5s.pt, imgsz=[640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, dynamic=True, simplify=False, opset=17, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['torchscript']\n",
            "YOLOv5 üöÄ v7.0-241-gb6a65e1 Python-3.10.12 torch-2.1.0+cu121 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from yolov5s.pt with output shape (1, 25200, 85) (14.1 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.1.0+cu121...\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success ‚úÖ 3.6s, saved as yolov5s.torchscript (27.9 MB)\n",
            "\n",
            "Export complete (4.9s)\n",
            "Results saved to \u001b[1m/content/gdrive/MyDrive/Winforms_data/yolov5\u001b[0m\n",
            "Detect:          python detect.py --weights yolov5s.torchscript \n",
            "Validate:        python val.py --weights yolov5s.torchscript \n",
            "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5s.torchscript')  \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e2fe8c14f00d>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Find the ONNX file in the export directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0monnx_temp_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yolov5s.onnx'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# The name generated by YOLOv5 export script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0monnx_temp_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monnx_export_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ONNX model exported to: {onnx_export_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/pathlib.py\u001b[0m in \u001b[0;36mrename\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mPath\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mpointing\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \"\"\"\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yolov5s.onnx' -> '/content/gdrive/MyDrive/Winforms_data/yolov5/onnx_exported_model.onnx'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Change directory to allow Google to access GDrive\n",
        "%cd /content/gdrive/MyDrive/Winforms_data\n",
        "\n",
        "# Clone YOLOv5 repository\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "\n",
        "# Change directory to the YOLOv5 repository\n",
        "%cd yolov5\n",
        "\n",
        "# Install required packages\n",
        "%pip install -qr requirements.txt comet_ml\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "# Define the zip file name\n",
        "zip_file_name = 'chickenDB.zip'  # Change this to the actual zip file name\n",
        "\n",
        "# Extract the base name of the zip file (excluding the extension)\n",
        "base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
        "\n",
        "# Update the YAML file with the extracted base name\n",
        "yaml_content = f\"\"\"\n",
        "train: /content/gdrive/MyDrive/Winforms_data/yolov5/dataset_extract/{base_name}/images/train/\n",
        "val: /content/gdrive/MyDrive/Winforms_data/yolov5/dataset_extract/{base_name}/images/val/\n",
        "nc: 1\n",
        "names: ['{base_name}']\n",
        "\"\"\"\n",
        "\n",
        "# Write the updated YAML content to a file\n",
        "yaml_file_path = '/content/gdrive/MyDrive/Winforms_data/yolov5/data/defaultYamlpath.yaml'\n",
        "with open(yaml_file_path, 'w') as yaml_file:\n",
        "    yaml_file.write(yaml_content)\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "extracted_folder_path = f'/content/gdrive/MyDrive/Winforms_data/yolov5/dataset_extract'\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the zip file and the destination folder\n",
        "zip_file_path = f'/content/gdrive/MyDrive/Winforms_data/yolov5/zipfiles/{zip_file_name}'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(f\" '{zip_file_name}' extracted to '{extracted_folder_path}'.\")\n",
        "\n",
        "# Make label code\n",
        "# training\n",
        "# Train YOLOv5s on marmot for 3 epochs\n",
        "!python train.py --img 640 --batch 21 --epochs 2 --data {yaml_file_path} --weights yolov5s.pt --cache\n",
        "\n",
        "# After training, export the model to ONNX format\n",
        "model_path = 'runs/train/exp/weights/best.pt'  # Change this path based on your training output\n",
        "onnx_export_path = '/content/gdrive/MyDrive/Winforms_data/yolov5/onnx_exported_model.onnx'\n",
        "\n",
        "\n",
        "# Export the trained model to ONNX format\n",
        "!python export.py --img-size 640 --batch-size 1 --dynamic\n",
        "\n",
        "# Find the ONNX file in the export directory\n",
        "onnx_temp_path = Path('yolov5s.onnx')  # The name generated by YOLOv5 export script\n",
        "onnx_temp_path.rename(onnx_export_path)\n",
        "\n",
        "print(f\"ONNX model exported to: {onnx_export_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY-_qIfAh8hg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Set the working directory\n",
        "working_directory = '/content/gdrive/MyDrive/Winforms_data/yolov5'\n",
        "os.chdir(working_directory)\n",
        "\n",
        "# Function to read parameters from the text file\n",
        "def read_parameters(file_path):\n",
        "    parameters = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            key, value = line.strip().split(':')\n",
        "            parameters[key.strip()] = value.strip()\n",
        "    return parameters\n",
        "\n",
        "# Define the zip file name\n",
        "zip_file_name = 'chickenDB.zip'\n",
        "\n",
        "# Extract the base name of the zip file (excluding the extension)\n",
        "base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
        "\n",
        "# Update the YAML file with the extracted base name\n",
        "yaml_content = f\"\"\"\n",
        "train: {working_directory}/dataset_extract/{base_name}/images/train/\n",
        "val: {working_directory}/dataset_extract/{base_name}/images/val/\n",
        "nc: 1\n",
        "names: ['{base_name}']\n",
        "\"\"\"\n",
        "\n",
        "# Write the updated YAML content to a file\n",
        "yaml_file_path = f'{working_directory}/data/defaultYamlpath.yaml'\n",
        "with open(yaml_file_path, 'w') as yaml_file:\n",
        "    yaml_file.write(yaml_content)\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "extracted_folder_path = f'{working_directory}/dataset_extract'\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the zip file and the destination folder\n",
        "zip_file_path = f'{working_directory}/zipfiles/{zip_file_name}'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(f\" '{zip_file_name}' extracted to '{extracted_folder_path}'.\")\n",
        "\n",
        "# Read user parameters from the text file\n",
        "text_file_path = f'{working_directory}/zipfiles/{base_name}.txt'\n",
        "if not os.path.exists(text_file_path):\n",
        "    print(f\"Error: Text file '{base_name}.txt' not found.\")\n",
        "else:\n",
        "    parameters = read_parameters(text_file_path)\n",
        "\n",
        "    # Check if required parameters are present\n",
        "    required_params = ['img', 'batch', 'epochs', 'weights']\n",
        "    missing_params = [param for param in required_params if param not in parameters]\n",
        "\n",
        "    if missing_params:\n",
        "        print(f\"Error: Missing parameters in text file: {', '.join(missing_params)}\")\n",
        "    else:\n",
        "\n",
        "        # Train YOLOv5s with user parameters\n",
        "        #train_command = f\"!python train.py --img {parameters['img']} --batch {parameters['batch']} --epochs {parameters['epochs']} \\n --data {yaml_file_path} --weights {parameters['weights']} --cache \\n\"\n",
        "\n",
        "        !python train.py --img {parameters['img']}  --batch {parameters['batch']} --epochs {parameters['epochs']} --data {yaml_file_path} --weights {parameters['weights']} --cache\n",
        "\n",
        "       # print(f\"Training command: {train_command}\")\n",
        "\n",
        "        # Debugging: Print the current working directory\n",
        "       # print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "        # Debug\n",
        "        #print(\"Training command:\", train_command)\n",
        "\n",
        "        # Run\n",
        "        #os.system(train_command)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}