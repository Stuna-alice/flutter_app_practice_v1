{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stuna-alice/flutter_app_practice_v1/blob/master/C_backend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "from models.yolo import Model\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# Set the working directory\n",
        "working_directory = '/content/gdrive/MyDrive/Winforms_data/yolov5'\n",
        "\n",
        "os.chdir(working_directory)\n",
        "\n",
        "# Function to read parameters from the text file\n",
        "def read_parameters(file_path):\n",
        "    parameters = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            # Check if the line contains a colon\n",
        "            if ':' in line:\n",
        "                key, value = line.strip().split(':', 1)\n",
        "                parameters[key.strip()] = value.strip()\n",
        "    return parameters\n",
        "\n",
        "# Define the zip file name\n",
        "zip_file_name = 'chickenDB.zip'\n",
        "\n",
        "# Extract the base name of the zip file (excluding the extension)\n",
        "base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
        "\n",
        "# Update the YAML file with the extracted base name\n",
        "yaml_content = f\"\"\"\n",
        "train: {working_directory}/dataset_extract/{base_name}/images/train/\n",
        "val: {working_directory}/dataset_extract/{base_name}/images/val/\n",
        "nc: 1\n",
        "names: ['{base_name}']\n",
        "\"\"\"\n",
        "\n",
        "# Write the updated YAML content to a file\n",
        "yaml_file_path = f'{working_directory}/data/defaultYamlpath.yaml'\n",
        "with open(yaml_file_path, 'w') as yaml_file:\n",
        "    yaml_file.write(yaml_content)\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "extracted_folder_path = f'{working_directory}/dataset_extract'\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the zip file and the destination folder\n",
        "zip_file_path = f'{working_directory}/zipfiles/{zip_file_name}'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(f\"'{zip_file_name}' extracted to '{extracted_folder_path}'.\")\n",
        "\n",
        "# Read user parameters from the text file\n",
        "text_file_path = f'{working_directory}/zipfiles/{base_name}.txt'\n",
        "if not os.path.exists(text_file_path):\n",
        "    print(f\"Error: Text file '{base_name}.txt' not found.\")\n",
        "else:\n",
        "    parameters = read_parameters(text_file_path)\n",
        "\n",
        "    # Print all parameters\n",
        "    print(\"All Parameters:\")\n",
        "    for key, value in parameters.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    # Check if required parameters are present\n",
        "    required_params = ['img', 'batch', 'epochs', 'weights']\n",
        "    missing_params = [param for param in required_params if param not in parameters]\n",
        "\n",
        "    if missing_params:\n",
        "        print(f\"Error: Missing parameters in text file: {', '.join(missing_params)}\")\n",
        "    else:\n",
        "        # Train YOLOv5s with user parameters\n",
        "        weights_path = parameters['weights']\n",
        "        !python train.py --img {parameters['img']} --batch {parameters['batch']} --epochs {parameters['epochs']} --data {yaml_file_path} --weights {weights_path} --cache\n",
        "\n",
        "        # Load the YOLOv5 model from yolov5s.yaml\n",
        "        model_config_path = '/content/gdrive/MyDrive/Winforms_data/yolov5/models/yolov5s.yaml'\n",
        "        model = Model(model_config_path)  # Replace with the actual code to instantiate the YOLOv5 model\n",
        "\n",
        "        model_name = f\"{base_name}.pth\"\n",
        "        weights_dir = os.path.join(working_directory, 'weights')\n",
        "\n",
        "        # Create the 'weights' directory if it doesn't exist\n",
        "        os.makedirs(weights_dir, exist_ok=True)\n",
        "\n",
        "        model_path = os.path.join(weights_dir, model_name)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        # Convert the .pth file to ONNX format\n",
        "        onnx_path = f'{working_directory}/zipfiles/{base_name}.onnx'\n",
        "        dummy_input = torch.randn(int(parameters['batch']), 3, int(parameters['img']), int(parameters['img']))\n",
        "        torch.onnx.export(model, dummy_input, onnx_path)\n",
        "\n",
        "        print(f\"Model saved as '{model_name}' and exported to '{onnx_path}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pvk1HxrSHDs",
        "outputId": "9b6d6938-170f-4d32-ff50-52f24648ac4b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 5, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 10, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n",
            "    from pip._internal.build_env import get_runnable_pip\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/build_env.py\", line 19, in <module>\n",
            "    from pip._internal.cli.spinners import open_spinner\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n",
            "    from pip._internal.utils.logging import get_indentation\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 13, in <module>\n",
            "    from pip._vendor.rich.console import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/__init__.py\", line 17, in <module>\n",
            "    _IMPORT_CWD = os.path.abspath(os.getcwd())\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "fatal: could not create work tree dir 'onnx-tensorflow': Transport endpoint is not connected\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 5, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 10, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n",
            "    from pip._internal.build_env import get_runnable_pip\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/build_env.py\", line 19, in <module>\n",
            "    from pip._internal.cli.spinners import open_spinner\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n",
            "    from pip._internal.utils.logging import get_indentation\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 13, in <module>\n",
            "    from pip._vendor.rich.console import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/__init__.py\", line 17, in <module>\n",
            "    _IMPORT_CWD = os.path.abspath(os.getcwd())\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "'chickenDB.zip' extracted to '/content/gdrive/MyDrive/Winforms_data/yolov5/dataset_extract'.\n",
            "All Parameters:\n",
            "img: 640\n",
            "batch: 1\n",
            "epochs: 2\n",
            "weights: yolov5s.pt\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/gdrive/MyDrive/Winforms_data/yolov5/data/defaultYamlpath.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=2, batch_size=1, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 29 commits. Use 'git pull' or 'git clone https://github.com/ultralytics/yolov5' to update.\n",
            "YOLOv5 üöÄ v7.0-241-gb6a65e1 Python-3.10.12 torch-2.1.0+cu121 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/Winforms_data/yolov5/dataset_extract/chickenDB/labels/train.cache... 46 images, 0 backgrounds, 0 corrupt: 100% 46/46 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram): 100% 46/46 [00:00<00:00, 46.35it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/Winforms_data/yolov5/dataset_extract/chickenDB/labels/val.cache... 28 images, 0 backgrounds, 0 corrupt: 100% 28/28 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100% 28/28 [00:00<00:00, 30.02it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m3.12 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
            "Plotting labels to runs/train/exp200/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp200\u001b[0m\n",
            "Starting training for 2 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        0/1         0G    0.08269    0.04012          0          6        640: 100% 46/46 [01:15<00:00,  1.64s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0% 0/14 [00:00<?, ?it/s]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  14% 2/14 [00:02<00:13,  1.16s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  21% 3/14 [00:03<00:14,  1.30s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  29% 4/14 [00:05<00:13,  1.40s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  36% 5/14 [00:06<00:12,  1.39s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  43% 6/14 [00:08<00:11,  1.46s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50% 7/14 [00:10<00:12,  1.75s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  57% 8/14 [00:12<00:11,  1.85s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  64% 9/14 [00:14<00:09,  1.85s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  71% 10/14 [00:16<00:06,  1.73s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  79% 11/14 [00:17<00:05,  1.79s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  86% 12/14 [00:19<00:03,  1.69s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  93% 13/14 [00:20<00:01,  1.53s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 14/14 [00:21<00:00,  1.56s/it]\n",
            "                   all         28         43    0.00895      0.163    0.00711    0.00176\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        1/1         0G    0.07143    0.03534          0          2        640: 100% 46/46 [01:06<00:00,  1.45s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0% 0/14 [00:00<?, ?it/s]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   7% 1/14 [00:01<00:23,  1.82s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  14% 2/14 [00:03<00:20,  1.72s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  21% 3/14 [00:04<00:17,  1.60s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  29% 4/14 [00:06<00:15,  1.55s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  36% 5/14 [00:07<00:13,  1.48s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  43% 6/14 [00:09<00:11,  1.48s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50% 7/14 [00:10<00:10,  1.46s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  57% 8/14 [00:12<00:09,  1.54s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  64% 9/14 [00:14<00:08,  1.73s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  71% 10/14 [00:16<00:07,  1.86s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  79% 11/14 [00:18<00:05,  1.89s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  93% 13/14 [00:21<00:01,  1.61s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 14/14 [00:22<00:00,  1.62s/it]\n",
            "                   all         28         43     0.0118      0.256    0.00838    0.00193\n",
            "\n",
            "2 epochs completed in 0.052 hours.\n",
            "Optimizer stripped from runs/train/exp200/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from runs/train/exp200/weights/best.pt, 14.4MB\n",
            "\n",
            "Validating runs/train/exp200/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  14% 2/14 [00:02<00:13,  1.11s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  21% 3/14 [00:03<00:13,  1.23s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  29% 4/14 [00:05<00:14,  1.49s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  36% 5/14 [00:07<00:13,  1.53s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  43% 6/14 [00:08<00:12,  1.57s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50% 7/14 [00:10<00:12,  1.74s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  57% 8/14 [00:12<00:10,  1.72s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  64% 9/14 [00:14<00:08,  1.70s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  71% 10/14 [00:15<00:06,  1.59s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  79% 11/14 [00:17<00:04,  1.65s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  93% 13/14 [00:19<00:01,  1.44s/it]WARNING ‚ö†Ô∏è NMS time limit 0.600s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 14/14 [00:21<00:00,  1.52s/it]\n",
            "                   all         28         43     0.0128      0.256    0.00912    0.00208\n",
            "Results saved to \u001b[1mruns/train/exp200\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "YOLOv5s summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
            "\n",
            "/content/gdrive/MyDrive/Winforms_data/yolov5/models/yolo.py:207: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if augment:\n",
            "/content/gdrive/MyDrive/Winforms_data/yolov5/models/yolo.py:119: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if profile:\n",
            "/content/gdrive/MyDrive/Winforms_data/yolov5/models/yolo.py:123: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if visualize:\n",
            "/content/gdrive/MyDrive/Winforms_data/yolov5/models/yolo.py:123: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if visualize:\n",
            "/content/gdrive/MyDrive/Winforms_data/yolov5/models/yolo.py:119: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if profile:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as 'chickenDB.pth' and exported to '/content/gdrive/MyDrive/Winforms_data/yolov5/zipfiles/chickenDB.onnx'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "from models.yolo import Model  # Replace with the actual import for the YOLOv5 model class\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# Set the working directory\n",
        "working_directory = '/content/gdrive/MyDrive/Winforms_data/yolov5'\n",
        "!pip install tensorflow-addons\n",
        "!git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow && pip install -e .\n",
        "!pip install onnx\n",
        "os.chdir(working_directory)\n",
        "\n",
        "# Function to read parameters from the text file\n",
        "def read_parameters(file_path):\n",
        "    parameters = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            # Check if the line contains a colon\n",
        "            if ':' in line:\n",
        "                key, value = line.strip().split(':', 1)\n",
        "                parameters[key.strip()] = value.strip()\n",
        "    return parameters\n",
        "\n",
        "# Define the zip file name\n",
        "zip_file_name = 'chickenDB.zip'\n",
        "\n",
        "# Extract the base name of the zip file (excluding the extension)\n",
        "base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
        "\n",
        "# Update the YAML file with the extracted base name\n",
        "yaml_content = f\"\"\"\n",
        "train: {working_directory}/dataset_extract/{base_name}/images/train/\n",
        "val: {working_directory}/dataset_extract/{base_name}/images/val/\n",
        "nc: 1\n",
        "names: ['{base_name}']\n",
        "\"\"\"\n",
        "\n",
        "# Write the updated YAML content to a file\n",
        "yaml_file_path = f'{working_directory}/data/defaultYamlpath.yaml'\n",
        "with open(yaml_file_path, 'w') as yaml_file:\n",
        "    yaml_file.write(yaml_content)\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "extracted_folder_path = f'{working_directory}/dataset_extract'\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the zip file and the destination folder\n",
        "zip_file_path = f'{working_directory}/zipfiles/{zip_file_name}'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(f\"'{zip_file_name}' extracted to '{extracted_folder_path}'.\")\n",
        "\n",
        "# Read user parameters from the text file\n",
        "text_file_path = f'{working_directory}/zipfiles/{base_name}.txt'\n",
        "if not os.path.exists(text_file_path):\n",
        "    print(f\"Error: Text file '{base_name}.txt' not found.\")\n",
        "else:\n",
        "    parameters = read_parameters(text_file_path)\n",
        "\n",
        "    # Print all parameters\n",
        "    print(\"All Parameters:\")\n",
        "    for key, value in parameters.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    # Check if required parameters are present\n",
        "    required_params = ['img', 'batch', 'epochs', 'weights']\n",
        "    missing_params = [param for param in required_params if param not in parameters]\n",
        "\n",
        "    if missing_params:\n",
        "        print(f\"Error: Missing parameters in text file: {', '.join(missing_params)}\")\n",
        "    else:\n",
        "        # Train YOLOv5s with user parameters\n",
        "        weights_path = parameters['weights']\n",
        "        !python train.py --img {parameters['img']} --batch {parameters['batch']} --epochs {parameters['epochs']} --data {yaml_file_path} --weights {weights_path} --cache\n",
        "\n",
        "        # Load the YOLOv5 model from yolov5s.yaml\n",
        "        model_config_path = '/content/gdrive/MyDrive/Winforms_data/yolov5/models/yolov5s.yaml'\n",
        "        model = Model(model_config_path)  # Replace with the actual code to instantiate the YOLOv5 model\n",
        "\n",
        "        # Save the trained model as a .pth file\n",
        "        model_name = f\"{base_name}.pth\"\n",
        "        model_path = os.path.join(working_directory, 'weights', model_name)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "        # Convert the .pth file to ONNX format\n",
        "        onnx_path = f'{working_directory}/zipfiles/{base_name}.onnx'\n",
        "        dummy_input = torch.randn(int(parameters['batch']), 3, int(parameters['img']), int(parameters['img']))\n",
        "        torch.onnx.export(model, dummy_input, onnx_path)\n",
        "\n",
        "        print(f\"Model saved as '{model_name}' and exported to '{onnx_path}'.\")\n"
      ],
      "metadata": {
        "id": "TrDMbq8DCzz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#version\n",
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# Set the working directory\n",
        "working_directory = '/content/gdrive/MyDrive/Winforms_data/yolov5'\n",
        "os.chdir(working_directory)\n",
        "\n",
        "# Function to read parameters from the text file\n",
        "def read_parameters(file_path):\n",
        "    parameters = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            # Check if the line contains a colon\n",
        "            if ':' in line:\n",
        "                key, value = line.strip().split(':', 1)\n",
        "                parameters[key.strip()] = value.strip()\n",
        "    return parameters\n",
        "\n",
        "# Define the zip file name\n",
        "zip_file_name = 'chickenDB.zip'\n",
        "\n",
        "# Extract the base name of the zip file (excluding the extension)\n",
        "base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
        "\n",
        "# Update the YAML file with the extracted base name\n",
        "yaml_content = f\"\"\"\n",
        "train: {working_directory}/dataset_extract/{base_name}/images/train/\n",
        "val: {working_directory}/dataset_extract/{base_name}/images/val/\n",
        "nc: 1\n",
        "names: ['{base_name}']\n",
        "\"\"\"\n",
        "\n",
        "# Write the updated YAML content to a file\n",
        "yaml_file_path = f'{working_directory}/data/defaultYamlpath.yaml'\n",
        "with open(yaml_file_path, 'w') as yaml_file:\n",
        "    yaml_file.write(yaml_content)\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "extracted_folder_path = f'{working_directory}/dataset_extract'\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the zip file and the destination folder\n",
        "zip_file_path = f'{working_directory}/zipfiles/{zip_file_name}'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(f\"'{zip_file_name}' extracted to '{extracted_folder_path}'.\")\n",
        "\n",
        "# Read user parameters from the text file\n",
        "text_file_path = f'{working_directory}/zipfiles/{base_name}.txt'\n",
        "if not os.path.exists(text_file_path):\n",
        "    print(f\"Error: Text file '{base_name}.txt' not found.\")\n",
        "else:\n",
        "    parameters = read_parameters(text_file_path)\n",
        "\n",
        "    # Print all parameters\n",
        "    print(\"All Parameters:\")\n",
        "    for key, value in parameters.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    # Check if required parameters are present\n",
        "    required_params = ['img', 'batch', 'epochs', 'weights']\n",
        "    missing_params = [param for param in required_params if param not in parameters]\n",
        "\n",
        "    if missing_params:\n",
        "        print(f\"Error: Missing parameters in text file: {', '.join(missing_params)}\")\n",
        "    else:\n",
        "        # Define your Net class\n",
        "        class Net(nn.Module):\n",
        "            def __init__(self):\n",
        "                super(Net, self).__init__()\n",
        "                self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "                self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "                self.conv2_drop = nn.Dropout2d()\n",
        "                self.fc1 = nn.Linear(320, 50)\n",
        "                self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "                x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "                x = x.view(-1, 320)\n",
        "                x = F.relu(self.fc1(x))\n",
        "                x = F.dropout(x, training=self.training)\n",
        "                x = self.fc2(x)\n",
        "                return F.log_softmax(x, dim=1)\n",
        "\n",
        "        # Define the train function\n",
        "        def train(model, device, train_loader, optimizer, epoch):\n",
        "            model.train()\n",
        "            for batch_idx, (data, target) in enumerate(train_loader):\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                output = model(data)\n",
        "                loss = F.nll_loss(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                if batch_idx % 1000 == 0:\n",
        "                    print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
        "\n",
        "        # Train Net with user parameters\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "        train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=int(parameters['batch']), shuffle=True)\n",
        "\n",
        "        net = Net().to(device)\n",
        "        optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "        for epoch in range(int(parameters['epochs'])):\n",
        "            train(net, device, train_loader, optimizer, epoch)\n",
        "\n",
        "        # Save the trained model\n",
        "        model_path = f'{working_directory}/{base_name}.pth'\n",
        "        torch.save(net.state_dict(), model_path)\n",
        "\n",
        "        # Load the saved model\n",
        "        trained_model = Net().to(device)\n",
        "        trained_model.load_state_dict(torch.load(model_path))\n",
        "        trained_model.eval()\n",
        "\n",
        "        # Export the trained model to ONNX format\n",
        "        dummy_input = Variable(torch.randn(int(parameters['batch']), 1, int(parameters['img']), int(parameters['img'])))\n",
        "        onnx_path = f'{working_directory}/zipfiles/{base_name}.onnx'\n",
        "        torch.onnx.export(trained_model, dummy_input, onnx_path)\n",
        "\n",
        "        print(f\"Model saved to '{model_path}'.\")\n",
        "        print(f\"ONNX file exported to '{onnx_path}'.\")\n"
      ],
      "metadata": {
        "id": "ZOC_gev_y2df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "07595bb8-c01e-41eb-c6fc-b1824263e49a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-7fbc8d595f5e>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Extract the contents of the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_folder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{zip_file_name}' extracted to '{extracted_folder_path}'.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1700\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mfdst_write\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 995\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_read2\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    745\u001b[0m                         \"Close the writing handle before trying to read.\")\n\u001b[1;32m    746\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlC6AYFHzdkZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import subprocess\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# Set the working directory\n",
        "working_directory = '/content/gdrive/MyDrive/Winforms_data/yolov5'\n",
        "os.chdir(working_directory)\n",
        "\n",
        "# Function to read parameters from the text file\n",
        "def read_parameters(file_path):\n",
        "    parameters = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            # Check if the line contains a colon\n",
        "            if ':' in line:\n",
        "                key, value = line.strip().split(':', 1)\n",
        "                parameters[key.strip()] = value.strip()\n",
        "    return parameters\n",
        "\n",
        "# Define the zip file name\n",
        "zip_file_name = 'chickenDB.zip'\n",
        "\n",
        "# Extract the base name of the zip file (excluding the extension)\n",
        "base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
        "\n",
        "# Update the YAML file with the extracted base name\n",
        "yaml_content = f\"\"\"\n",
        "train: {working_directory}/dataset_extract/{base_name}/images/train/\n",
        "val: {working_directory}/dataset_extract/{base_name}/images/val/\n",
        "nc: 1\n",
        "names: ['{base_name}']\n",
        "\"\"\"\n",
        "\n",
        "# Write the updated YAML content to a file\n",
        "yaml_file_path = f'{working_directory}/data/defaultYamlpath.yaml'\n",
        "with open(yaml_file_path, 'w') as yaml_file:\n",
        "    yaml_file.write(yaml_content)\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "extracted_folder_path = f'{working_directory}/dataset_extract'\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the zip file and the destination folder\n",
        "zip_file_path = f'{working_directory}/zipfiles/{zip_file_name}'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(f\"'{zip_file_name}' extracted to '{extracted_folder_path}'.\")\n",
        "\n",
        "# Read user parameters from the text file\n",
        "text_file_path = f'{working_directory}/zipfiles/{base_name}.txt'\n",
        "if not os.path.exists(text_file_path):\n",
        "    print(f\"Error: Text file '{base_name}.txt' not found.\")\n",
        "else:\n",
        "    parameters = read_parameters(text_file_path)\n",
        "\n",
        "    # Print all parameters\n",
        "    print(\"All Parameters:\")\n",
        "    for key, value in parameters.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    # Check if required parameters are present\n",
        "    required_params = ['img', 'batch', 'epochs', 'weights']\n",
        "    missing_params = [param for param in required_params if param not in parameters]\n",
        "\n",
        "    if missing_params:\n",
        "        print(f\"Error: Missing parameters in text file: {', '.join(missing_params)}\")\n",
        "    else:\n",
        "        # Export the trained model to ONNX format after training\n",
        "        export_script_path = os.path.join(working_directory, 'export.py')\n",
        "        if not os.path.exists(export_script_path):\n",
        "            print(f\"Error: 'export.py' not found in the specified directory.\")\n",
        "        else:\n",
        "            # Print the current working directory\n",
        "            print(\"Current Working Directory:\", os.getcwd())\n",
        "\n",
        "            base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
        "            # Export the trained model to ONNX format after training\n",
        "        export_script_path = os.path.join(working_directory, 'export.py')\n",
        "        if not os.path.exists(export_script_path):\n",
        "            print(f\"Error: 'export.py' not found in the specified directory.\")\n",
        "        else:\n",
        "            # Print the current working directory\n",
        "            print(\"Current Working Directory:\", os.getcwd())\n",
        "\n",
        "            # Train YOLOv5s with user parameters\n",
        "            weights_path = parameters['weights']\n",
        "            !python train.py --img {parameters['img']} --batch {parameters['batch']} --epochs {parameters['epochs']} --data {yaml_file_path} --weights {weights_path} --cache\n",
        "\n",
        "            # Export the trained model to ONNX format after training\n",
        "            output_onnx_path = f'{working_directory}/zipfiles/{base_name}.onnx'\n",
        "            export_cmd = f\"python {export_script_path} --weights {weights_path} --img-size {parameters['img']} --dynamic --simplify {output_onnx_path}\"\n",
        "\n",
        "            try:\n",
        "                print(\"Executing export command:\", export_cmd)\n",
        "                os.system(export_cmd)\n",
        "                print(f\"ONNX file exported to '{output_onnx_path}'.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error: Export command failed with message: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_Q5923pmyAGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k605ep2DkfRn",
        "outputId": "c6eeb67c-97f2-4ebd-8f18-fcdba7226451"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-e12d94d25631>\", line 6, in <cell line: 6>\n",
            "    import torch\n",
            "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1002, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 945, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1439, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1408, in _get_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1366, in _path_importer_cache\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-e12d94d25631>\", line 6, in <cell line: 6>\n",
            "    import torch\n",
            "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1002, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 945, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1439, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1408, in _get_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1366, in _path_importer_cache\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-e12d94d25631>\", line 6, in <cell line: 6>\n",
            "    import torch\n",
            "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1002, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 945, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1439, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1408, in _get_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1366, in _path_importer_cache\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n",
            "    self.showtraceback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
            "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import check_img_size\n",
        "\n",
        "# Set the working directory\n",
        "working_directory = '/content/gdrive/MyDrive/Winforms_data/yolov5'\n",
        "os.chdir(working_directory)\n",
        "\n",
        "# Function to read parameters from the text file\n",
        "def read_parameters(file_path):\n",
        "    parameters = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            key, value = line.strip().split(':')\n",
        "            parameters[key.strip()] = value.strip()\n",
        "    return parameters\n",
        "\n",
        "# Define the zip file name\n",
        "zip_file_name = 'chickenDB.zip'\n",
        "\n",
        "# Extract the base name of the zip file (excluding the extension)\n",
        "base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
        "\n",
        "# Update the YAML file with the extracted base name\n",
        "yaml_content = f\"\"\"\n",
        "train: {working_directory}/dataset_extract/{base_name}/images/train/\n",
        "val: {working_directory}/dataset_extract/{base_name}/images/val/\n",
        "nc: 1\n",
        "names: ['{base_name}']\n",
        "\"\"\"\n",
        "\n",
        "# Write the updated YAML content to a file\n",
        "yaml_file_path = f'{working_directory}/data/defaultYamlpath.yaml'\n",
        "with open(yaml_file_path, 'w') as yaml_file:\n",
        "    yaml_file.write(yaml_content)\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "extracted_folder_path = f'{working_directory}/dataset_extract'\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the zip file and the destination folder\n",
        "zip_file_path = f'{working_directory}/zipfiles/{zip_file_name}'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(f\" '{zip_file_name}' extracted to '{extracted_folder_path}'.\")\n",
        "\n",
        "# Read user parameters from the text file\n",
        "text_file_path = f'{working_directory}/zipfiles/{base_name}.txt'\n",
        "if not os.path.exists(text_file_path):\n",
        "    print(f\"Error: Text file '{base_name}.txt' not found.\")\n",
        "else:\n",
        "    parameters = read_parameters(text_file_path)\n",
        "\n",
        "    # Check if required parameters are present\n",
        "    required_params = ['img', 'batch', 'epochs', 'weights']\n",
        "    missing_params = [param for param in required_params if param not in parameters]\n",
        "\n",
        "    if missing_params:\n",
        "        print(f\"Error: Missing parameters in text file: {', '.join(missing_params)}\")\n",
        "    else:\n",
        "        # Train YOLOv5 with user parameters\n",
        "        !python train.py --img {parameters['img']} --batch {parameters['batch']} --epochs {parameters['epochs']} --data {yaml_file_path} --weights {parameters['weights']} --cache\n",
        "\n",
        "        # Load the trained YOLOv5 model\n",
        "        model = attempt_load(parameters['weights'], map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "        img_size = check_img_size(640, s=model.stride.max())  # Change 640 to your desired image size\n",
        "\n",
        "        # Export the model to ONNX\n",
        "        onnx_path = f'{working_directory}/yolov5.onnx'\n",
        "        model.model[-1].export = torch.jit.ExportType.ONNX\n",
        "        model.model[-1].optimize_for_inference(img_size)\n",
        "        model.model[-1].export.export_path = onnx_path\n",
        "        model.model[-1].export.image_size = img_size\n",
        "        model.model[-1].export.single_image = True\n",
        "        model.model[-1].forward = model.model[-1].export.forward\n",
        "        model.model[-1].export_module(torch.rand(1, 3, img_size, img_size).to(model.model[-1].device), verbose=True)\n",
        "\n",
        "        print(f\"Model exported to ONNX: {onnx_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x18KrmCb4JFe"
      },
      "outputs": [],
      "source": [
        "\n",
        "#running\n",
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import subprocess\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# Set the working directory\n",
        "working_directory = '/content/gdrive/MyDrive/Winforms_data/yolov5'\n",
        "os.chdir(working_directory)\n",
        "\n",
        "# Function to read parameters from the text file\n",
        "def read_parameters(file_path):\n",
        "    parameters = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            # Check if the line contains a colon\n",
        "            if ':' in line:\n",
        "                key, value = line.strip().split(':', 1)\n",
        "                parameters[key.strip()] = value.strip()\n",
        "    return parameters\n",
        "\n",
        "# Define the zip file name\n",
        "zip_file_name = 'chickenDB.zip'\n",
        "\n",
        "# Extract the base name of the zip file (excluding the extension)\n",
        "base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
        "\n",
        "# Update the YAML file with the extracted base name\n",
        "yaml_content = f\"\"\"\n",
        "train: {working_directory}/dataset_extract/{base_name}/images/train/\n",
        "val: {working_directory}/dataset_extract/{base_name}/images/val/\n",
        "nc: 1\n",
        "names: ['{base_name}']\n",
        "\"\"\"\n",
        "\n",
        "# Write the updated YAML content to a file\n",
        "yaml_file_path = f'{working_directory}/data/defaultYamlpath.yaml'\n",
        "with open(yaml_file_path, 'w') as yaml_file:\n",
        "    yaml_file.write(yaml_content)\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "extracted_folder_path = f'{working_directory}/dataset_extract'\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the zip file and the destination folder\n",
        "zip_file_path = f'{working_directory}/zipfiles/{zip_file_name}'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(f\"'{zip_file_name}' extracted to '{extracted_folder_path}'.\")\n",
        "\n",
        "# Read user parameters from the text file\n",
        "text_file_path = f'{working_directory}/zipfiles/{base_name}.txt'\n",
        "if not os.path.exists(text_file_path):\n",
        "    print(f\"Error: Text file '{base_name}.txt' not found.\")\n",
        "else:\n",
        "    parameters = read_parameters(text_file_path)\n",
        "\n",
        "    # Print all parameters\n",
        "    print(\"All Parameters:\")\n",
        "    for key, value in parameters.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    # Check if required parameters are present\n",
        "    required_params = ['img', 'batch', 'epochs', 'weights']\n",
        "    missing_params = [param for param in required_params if param not in parameters]\n",
        "\n",
        "    if missing_params:\n",
        "        print(f\"Error: Missing parameters in text file: {', '.join(missing_params)}\")\n",
        "    else:\n",
        "        # Train YOLOv5s with user parameters\n",
        "        weights_path = parameters['weights']\n",
        "        training_cmd = \"python train.py --img {img} --batch {batch} --epochs {epochs} --data {data} --weights {weights} --cache\".format(\n",
        "            img=parameters['img'],\n",
        "            batch=parameters['batch'],\n",
        "            epochs=parameters['epochs'],\n",
        "            data=yaml_file_path,\n",
        "            weights=weights_path\n",
        "        )\n",
        "        print(f\"Training command: {training_cmd}\")\n",
        "\n",
        "        # Execute training command using subprocess.run\n",
        "        subprocess.run(training_cmd, shell=True)\n",
        "\n",
        "        # Export the trained model to ONNX format\n",
        "        output_onnx_path = f'{working_directory}/zipfiles/{base_name}.onnx'\n",
        "        onnx_export_cmd = \"python export.py --weights {weights} --img-size {img_size} --include pb --dynamic --simplify --optimize 0 --simplify-num-ends 0 --simplify-num-mid 0 --simplify-num-start 0 --simplify-threshold 0.5 --simplify-method 0 --simplify-img-size {img_size} --simplify-nc 1 --dynamic-nc --simplify-preserve-reduction --output {output_path}\".format(\n",
        "            weights=weights_path,\n",
        "            img_size=parameters['img'],\n",
        "            output_path=output_onnx_path\n",
        "        )\n",
        "\n",
        "        print(f\"Exporting model to ONNX. Command: {onnx_export_cmd}\")\n",
        "\n",
        "        try:\n",
        "            # Execute ONNX export command using subprocess.run\n",
        "            subprocess.run(onnx_export_cmd, shell=True)\n",
        "            print(f\"Export successful.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during ONNX export: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRoO7Kz9qSI7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import subprocess\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# Set the working directory\n",
        "working_directory = '/content/gdrive/MyDrive/Winforms_data/yolov5'\n",
        "os.chdir(working_directory)\n",
        "\n",
        "# Function to read parameters from the text file\n",
        "def read_parameters(file_path):\n",
        "    parameters = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            # Check if the line contains a colon\n",
        "            if ':' in line:\n",
        "                key, value = line.strip().split(':', 1)\n",
        "                parameters[key.strip()] = value.strip()\n",
        "    return parameters\n",
        "\n",
        "# Define the zip file name\n",
        "zip_file_name = 'chickenDB.zip'\n",
        "\n",
        "# Extract the base name of the zip file (excluding the extension)\n",
        "base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
        "\n",
        "# Update the YAML file with the extracted base name\n",
        "yaml_content = f\"\"\"\n",
        "train: {working_directory}/dataset_extract/{base_name}/images/train/\n",
        "val: {working_directory}/dataset_extract/{base_name}/images/val/\n",
        "nc: 1\n",
        "names: ['{base_name}']\n",
        "\"\"\"\n",
        "\n",
        "# Write the updated YAML content to a file\n",
        "yaml_file_path = f'{working_directory}/data/defaultYamlpath.yaml'\n",
        "with open(yaml_file_path, 'w') as yaml_file:\n",
        "    yaml_file.write(yaml_content)\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "extracted_folder_path = f'{working_directory}/dataset_extract'\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the zip file and the destination folder\n",
        "zip_file_path = f'{working_directory}/zipfiles/{zip_file_name}'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(f\"'{zip_file_name}' extracted to '{extracted_folder_path}'.\")\n",
        "\n",
        "# Read user parameters from the text file\n",
        "text_file_path = f'{working_directory}/zipfiles/{base_name}.txt'\n",
        "if not os.path.exists(text_file_path):\n",
        "    print(f\"Error: Text file '{base_name}.txt' not found.\")\n",
        "else:\n",
        "    parameters = read_parameters(text_file_path)\n",
        "\n",
        "    # Print all parameters\n",
        "    print(\"All Parameters:\")\n",
        "    for key, value in parameters.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    # Check if required parameters are present\n",
        "    required_params = ['img', 'batch', 'epochs', 'weights']\n",
        "    missing_params = [param for param in required_params if param not in parameters]\n",
        "\n",
        "    if missing_params:\n",
        "        print(f\"Error: Missing parameters in text file: {', '.join(missing_params)}\")\n",
        "    else:\n",
        "        # Train YOLOv5s with user parameters\n",
        "        weights_path = parameters['weights']\n",
        "        !python train.py --img {parameters['img']} --batch {parameters['batch']} --epochs {parameters['epochs']} --data {yaml_file_path} --weights {weights_path} --cache\n",
        "\n",
        "        # Check if the export.py script exists\n",
        "        export_script_path = os.path.join(working_directory, 'export.py')\n",
        "        if not os.path.exists(export_script_path):\n",
        "            print(f\"Error: 'export.py' not found in the specified directory.\")\n",
        "        else:\n",
        "            # Export the trained model to ONNX format\n",
        "            output_onnx_path = f'{working_directory}/{base_name}.onnx'\n",
        "            onnx_export_cmd = f\"python {export_script_path} --weights {weights_path} --img-size {parameters['img']} --include pb --dynamic --simplify --optimize 0 --simplify-num-ends 0 --simplify-num-mid 0 --simplify-num-start 0 --simplify-threshold 0.5 --simplify-method 0 --simplify-img-size {parameters['img']} --simplify-nc 1 --dynamic-nc --simplify-preserve-reduction --output {output_onnx_path}\"\n",
        "\n",
        "            # Check if the export command was successful\n",
        "            exit_code = os.system(onnx_export_cmd)\n",
        "\n",
        "            # Capture the output of the command\n",
        "            onnx_export_output = os.popen(onnx_export_cmd).read()\n",
        "\n",
        "            if exit_code == 0:\n",
        "                print(f\"ONNX file exported to '{output_onnx_path}'.\")\n",
        "            else:\n",
        "                print(f\"Error: Export command exited with code {exit_code}.\")\n",
        "                print(f\"Export command output:\\n{onnx_export_output}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpAv5sHVjaJA"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# Set the working directory\n",
        "working_directory = '/content/gdrive/MyDrive/Winforms_data/yolov5'\n",
        "os.chdir(working_directory)\n",
        "\n",
        "# Function to read parameters from the text file\n",
        "def read_parameters(file_path):\n",
        "    parameters = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            key, value = line.strip().split(':')\n",
        "            parameters[key.strip()] = value.strip()\n",
        "    return parameters\n",
        "\n",
        "# Define the zip file name\n",
        "zip_file_name = 'chickenDB.zip'\n",
        "\n",
        "# Extract the base name of the zip file (excluding the extension)\n",
        "base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
        "\n",
        "# Update the YAML file with the extracted base name\n",
        "yaml_content = f\"\"\"\n",
        "train: {working_directory}/dataset_extract/{base_name}/images/train/\n",
        "val: {working_directory}/dataset_extract/{base_name}/images/val/\n",
        "nc: 1\n",
        "names: ['{base_name}']\n",
        "\"\"\"\n",
        "\n",
        "# Write the updated YAML content to a file\n",
        "yaml_file_path = f'{working_directory}/data/defaultYamlpath.yaml'\n",
        "with open(yaml_file_path, 'w') as yaml_file:\n",
        "    yaml_file.write(yaml_content)\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "extracted_folder_path = f'{working_directory}/dataset_extract'\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the zip file and the destination folder\n",
        "zip_file_path = f'{working_directory}/zipfiles/{zip_file_name}'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(f\" '{zip_file_name}' extracted to '{extracted_folder_path}'.\")\n",
        "\n",
        "# Read user parameters from the text file\n",
        "text_file_path = f'{working_directory}/zipfiles/{base_name}.txt'\n",
        "if not os.path.exists(text_file_path):\n",
        "    print(f\"Error: Text file '{base_name}.txt' not found.\")\n",
        "else:\n",
        "    parameters = read_parameters(text_file_path)\n",
        "\n",
        "    # Check if required parameters are present\n",
        "    required_params = ['img', 'batch', 'epochs', 'weights']\n",
        "    missing_params = [param for param in required_params if param not in parameters]\n",
        "\n",
        "    if missing_params:\n",
        "        print(f\"Error: Missing parameters in text file: {', '.join(missing_params)}\")\n",
        "    else:\n",
        "        # Train YOLOv5s with user parameters\n",
        "        !python train.py --img {parameters['img']}  --batch {parameters['batch']} --epochs {parameters['epochs']} --data {yaml_file_path} --weights {parameters['weights']} --cache\n",
        "\n",
        "        # Export the trained model to ONNX format\n",
        "        onnx_file_name = f'{base_name}.onnx'\n",
        "        output_onnx_path = f'{working_directory}/{onnx_file_name}'\n",
        "        export_command = f\"python export.py --weights runs/train/exp/weights/best.pt --img-size {parameters['img']} --batch-size {parameters['batch']} --include pb\"  # Modify the export.py command based on your requirements\n",
        "\n",
        "        !{export_command}\n",
        "\n",
        "        # Move the exported ONNX file to the desired location\n",
        "        shutil.move('yolov5s.onnx', output_onnx_path)\n",
        "\n",
        "        print(f\"Trained model exported to '{output_onnx_path}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eQ-3FBot291L"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Change directory to allow Google to access GDrive\n",
        "%cd /content/gdrive/MyDrive/Winforms_data\n",
        "\n",
        "# Clone YOLOv5 repository\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "\n",
        "# Change directory to the YOLOv5 repository\n",
        "%cd yolov5\n",
        "\n",
        "# Install required packages\n",
        "%pip install -qr requirements.txt comet_ml\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "\n",
        "# Define the path to the zip file and the destination folder\n",
        "zipFile_name = 'chickenDB.zip'\n",
        "zip_file_path = '/content/gdrive/MyDrive/Winforms_data/yolov5/zipfiles/' + zipFile_name\n",
        "extracted_folder_path = '/content/gdrive/MyDrive/Winforms_data/yolov5/dataset_extract'\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(f\" '{zipFile_name}' extracted to '{extracted_folder_path}'.\")\n",
        "\n",
        "\n",
        "# make label code\n",
        "#training\n",
        "# Train YOLOv5s on marmot for 3 epochs\n",
        "!python train.py --img 640 --batch 20 --epochs 50 --data controlDB.yaml --weights yolov5s.pt --cache\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xUYGnB7hSSQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Change directory to allow Google to access GDrive\n",
        "%cd /content/gdrive/MyDrive/Winforms_data\n",
        "\n",
        "# Clone YOLOv5 repository\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "\n",
        "# Change directory to the YOLOv5 repository\n",
        "%cd yolov5\n",
        "\n",
        "# Install required packages\n",
        "%pip install -qr requirements.txt comet_ml\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "# Define the zip file name\n",
        "zip_file_name = 'chickenDB.zip'  # Change this to the actual zip file name\n",
        "\n",
        "# Extract the base name of the zip file (excluding the extension)\n",
        "base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
        "\n",
        "# Update the YAML file with the extracted base name\n",
        "yaml_content = f\"\"\"\n",
        "train: /content/gdrive/MyDrive/Winforms_data/yolov5/dataset_extract/{base_name}/images/train/\n",
        "val: /content/gdrive/MyDrive/Winforms_data/yolov5/dataset_extract/{base_name}/images/val/\n",
        "nc: 1\n",
        "names: ['{base_name}']\n",
        "\"\"\"\n",
        "\n",
        "# Write the updated YAML content to a file\n",
        "yaml_file_path = '/content/gdrive/MyDrive/Winforms_data/yolov5/data/defaultYamlpath.yaml'\n",
        "with open(yaml_file_path, 'w') as yaml_file:\n",
        "    yaml_file.write(yaml_content)\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "extracted_folder_path = f'/content/gdrive/MyDrive/Winforms_data/yolov5/dataset_extract'\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the zip file and the destination folder\n",
        "zip_file_path = f'/content/gdrive/MyDrive/Winforms_data/yolov5/zipfiles/{zip_file_name}'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(f\" '{zip_file_name}' extracted to '{extracted_folder_path}'.\")\n",
        "\n",
        "# Make label code\n",
        "# training\n",
        "# Train YOLOv5s on marmot for 3 epochs\n",
        "!python train.py --img 640 --batch 21 --epochs 2 --data {yaml_file_path} --weights yolov5s.pt --cache\n",
        "\n",
        "# After training, export the model to ONNX format\n",
        "model_path = 'runs/train/exp/weights/best.pt'  # Change this path based on your training output\n",
        "onnx_export_path = '/content/gdrive/MyDrive/Winforms_data/yolov5/onnx_exported_model.onnx'\n",
        "\n",
        "\n",
        "# Export the trained model to ONNX format\n",
        "!python export.py --img-size 640 --batch-size 1 --dynamic\n",
        "\n",
        "# Find the ONNX file in the export directory\n",
        "onnx_temp_path = Path('yolov5s.onnx')  # The name generated by YOLOv5 export script\n",
        "onnx_temp_path.rename(onnx_export_path)\n",
        "\n",
        "print(f\"ONNX model exported to: {onnx_export_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY-_qIfAh8hg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Set the working directory\n",
        "working_directory = '/content/gdrive/MyDrive/Winforms_data/yolov5'\n",
        "os.chdir(working_directory)\n",
        "\n",
        "# Function to read parameters from the text file\n",
        "def read_parameters(file_path):\n",
        "    parameters = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            key, value = line.strip().split(':')\n",
        "            parameters[key.strip()] = value.strip()\n",
        "    return parameters\n",
        "\n",
        "# Define the zip file name\n",
        "zip_file_name = 'chickenDB.zip'\n",
        "\n",
        "# Extract the base name of the zip file (excluding the extension)\n",
        "base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
        "\n",
        "# Update the YAML file with the extracted base name\n",
        "yaml_content = f\"\"\"\n",
        "train: {working_directory}/dataset_extract/{base_name}/images/train/\n",
        "val: {working_directory}/dataset_extract/{base_name}/images/val/\n",
        "nc: 1\n",
        "names: ['{base_name}']\n",
        "\"\"\"\n",
        "\n",
        "# Write the updated YAML content to a file\n",
        "yaml_file_path = f'{working_directory}/data/defaultYamlpath.yaml'\n",
        "with open(yaml_file_path, 'w') as yaml_file:\n",
        "    yaml_file.write(yaml_content)\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "extracted_folder_path = f'{working_directory}/dataset_extract'\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the zip file and the destination folder\n",
        "zip_file_path = f'{working_directory}/zipfiles/{zip_file_name}'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(f\" '{zip_file_name}' extracted to '{extracted_folder_path}'.\")\n",
        "\n",
        "# Read user parameters from the text file\n",
        "text_file_path = f'{working_directory}/zipfiles/{base_name}.txt'\n",
        "if not os.path.exists(text_file_path):\n",
        "    print(f\"Error: Text file '{base_name}.txt' not found.\")\n",
        "else:\n",
        "    parameters = read_parameters(text_file_path)\n",
        "\n",
        "    # Check if required parameters are present\n",
        "    required_params = ['img', 'batch', 'epochs', 'weights']\n",
        "    missing_params = [param for param in required_params if param not in parameters]\n",
        "\n",
        "    if missing_params:\n",
        "        print(f\"Error: Missing parameters in text file: {', '.join(missing_params)}\")\n",
        "    else:\n",
        "\n",
        "        # Train YOLOv5s with user parameters\n",
        "        #train_command = f\"!python train.py --img {parameters['img']} --batch {parameters['batch']} --epochs {parameters['epochs']} \\n --data {yaml_file_path} --weights {parameters['weights']} --cache \\n\"\n",
        "\n",
        "        !python train.py --img {parameters['img']}  --batch {parameters['batch']} --epochs {parameters['epochs']} --data {yaml_file_path} --weights {parameters['weights']} --cache\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}